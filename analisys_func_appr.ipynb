{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want here? I want everything to be in functions and like \"main\" part, that will trigger all functions above. So you can simply turn on/off any function to look at the results and also you can make changes inside functions without influencing all the code b'z it is incide the function and kinda isolated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import xgboost\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_test(train_csv_path='train.csv', test_csv_path='test.csv'):\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "    df_test = pd.read_csv(test_csv_path)\n",
    "    return df_train, df_test\n",
    "\n",
    "def transforming_train_by_hand(df_train, perform_target_transf, exclude_anomalies):\n",
    "    # in this exact case we have two houses with really huge living area and they have relatively small sale price. We need to exclude them as they are clearly outliers\n",
    "    if exclude_anomalies == True:\n",
    "        df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n",
    "    if perform_target_transf == True:\n",
    "        df_train['SalePrice'] = np.log1p(df_train[\"SalePrice\"])\n",
    "    return df_train\n",
    "\n",
    "def get_combined_data(df_train, df_test, target_col_name):\n",
    "    combined_data = pd.concat([df_train.drop(columns=target_col_name), df_test])\n",
    "    return combined_data\n",
    "\n",
    "def get_num_obj_col_names(df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        num_colnames = df.select_dtypes(include=numerics).columns\n",
    "        obj_colnames = [x for x in df.columns if x not in num_colnames]\n",
    "        return num_colnames, obj_colnames\n",
    "\n",
    "def handl_mis_vals(combined_data, misval_approach, misval_addit_values_dict):\n",
    "    if misval_approach == 'kaggle_by_hand':\n",
    "        # took it from here\n",
    "        # https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "        # starting from In [14]:\n",
    "        combined_data[\"PoolQC\"] = combined_data[\"PoolQC\"].fillna(\"None\")\n",
    "        combined_data[\"MiscFeature\"] = combined_data[\"MiscFeature\"].fillna(\"None\")\n",
    "        combined_data[\"Alley\"] = combined_data[\"Alley\"].fillna(\"None\")\n",
    "        combined_data[\"Fence\"] = combined_data[\"Fence\"].fillna(\"None\")\n",
    "        combined_data[\"FireplaceQu\"] = combined_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "        # interesting way to give missing values mean values of the Neighborhood\n",
    "        combined_data[\"LotFrontage\"] = combined_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "        for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "            combined_data[col] = combined_data[col].fillna('None')\n",
    "        for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "            combined_data[col] = combined_data[col].fillna(0)\n",
    "        for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "            combined_data[col] = combined_data[col].fillna(0)\n",
    "        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "            combined_data[col] = combined_data[col].fillna('None')\n",
    "        combined_data[\"MasVnrType\"] = combined_data[\"MasVnrType\"].fillna(\"None\")\n",
    "        combined_data[\"MasVnrArea\"] = combined_data[\"MasVnrArea\"].fillna(0)\n",
    "        combined_data['MSZoning'] = combined_data['MSZoning'].fillna(combined_data['MSZoning'].mode()[0])\n",
    "        combined_data = combined_data.drop(['Utilities'], axis=1)\n",
    "        combined_data[\"Functional\"] = combined_data[\"Functional\"].fillna(\"Typ\")\n",
    "        combined_data['Electrical'] = combined_data['Electrical'].fillna(combined_data['Electrical'].mode()[0])\n",
    "        combined_data['KitchenQual'] = combined_data['KitchenQual'].fillna(combined_data['KitchenQual'].mode()[0])\n",
    "        combined_data['Exterior1st'] = combined_data['Exterior1st'].fillna(combined_data['Exterior1st'].mode()[0])\n",
    "        combined_data['Exterior2nd'] = combined_data['Exterior2nd'].fillna(combined_data['Exterior2nd'].mode()[0])\n",
    "        combined_data['SaleType'] = combined_data['SaleType'].fillna(combined_data['SaleType'].mode()[0])\n",
    "        combined_data['MSSubClass'] = combined_data['MSSubClass'].fillna(\"None\")\n",
    "\n",
    "    elif misval_approach == 'threshold_elim':\n",
    "        mis_prc_threshold = misval_addit_values_dict['mis_prc_threshold']\n",
    "\n",
    "        mis_data_df = combined_data.isnull().sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "        mis_data_df.columns = ['col_name', 'cnt_missing_vals']\n",
    "        mis_data_df['mis_perc'] = mis_data_df['cnt_missing_vals'] / combined_data.shape[0]\n",
    "\n",
    "        # columns to keep due to threshold\n",
    "        cols_to_keep = list(mis_data_df[mis_data_df['mis_perc'] < (mis_prc_threshold/100)]['col_name'])\n",
    "        mis_data_threshold_df = mis_data_df[mis_data_df['col_name'].isin(cols_to_keep)]\n",
    "\n",
    "        # columns to keep due to threshold were we can still find missing values \n",
    "        cols_to_keep_with_missvals = mis_data_threshold_df[mis_data_threshold_df['mis_perc'] != 0]\n",
    "        combined_data = combined_data[cols_to_keep].copy()  \n",
    "        \n",
    "        num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "\n",
    "        # obj_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        # num_imputer = SimpleImputer(strategy='median')\n",
    "        num_imputer = SimpleImputer(strategy=misval_addit_values_dict['num_imp_strat'])\n",
    "        obj_imputer = SimpleImputer(strategy=misval_addit_values_dict['obj_imp_strat'])\n",
    "\n",
    "\n",
    "        combined_data_num_imp = pd.DataFrame(num_imputer.fit_transform(combined_data[num_colnames]), columns=combined_data[num_colnames].columns)\n",
    "        combined_data_obj_imp = pd.DataFrame(obj_imputer.fit_transform(combined_data[obj_colnames]), columns=combined_data[obj_colnames].columns)\n",
    "\n",
    "        combined_data = pd.concat([combined_data_num_imp, combined_data_obj_imp], axis=1)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def data_add_transf(combined_data, add_transf_method):\n",
    "    if add_transf_method == 'kaggle_by_hand':\n",
    "        combined_data['MSSubClass'] = combined_data['MSSubClass'].apply(str)\n",
    "        combined_data['OverallCond'] = combined_data['OverallCond'].astype(str)\n",
    "        combined_data['YrSold'] = combined_data['YrSold'].astype(str)\n",
    "        combined_data['MoSold'] = combined_data['MoSold'].astype(str)\n",
    "\n",
    "        combined_data['TotalSF'] = combined_data['TotalBsmtSF'] + combined_data['1stFlrSF'] + combined_data['2ndFlrSF']\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def transform_skewed_feats(combined_data, num_colnames, skew_threshold):\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = combined_data[num_colnames].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "    skewness = skewness[abs(skewness) > skew_threshold]\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in [x for x in skewed_features if x != 'Id']:\n",
    "        combined_data[feat] = boxcox1p(combined_data[feat], lam)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def get_train_test_dfs_from_combined_data(df_train, target_col_name, combined_data):\n",
    "    new_df_train = combined_data[:df_train.shape[0]].copy()\n",
    "    new_df_test = combined_data[df_train.shape[0]:].copy()\n",
    "    new_df_train[target_col_name] = df_train[target_col_name]\n",
    "\n",
    "    return new_df_train, new_df_test\n",
    "\n",
    "def elim_cor_values(df_train, num_colnames, cor_threshold, target_col_name, combined_data):\n",
    "    num_and_targ_colnames = list(num_colnames)\n",
    "    num_and_targ_colnames.append(target_col_name)\n",
    "\n",
    "    new_df_train, new_df_test = get_train_test_dfs_from_combined_data(df_train, target_col_name, combined_data)\n",
    "\n",
    "    corr_matrix = new_df_train[num_and_targ_colnames].corr()\n",
    "\n",
    "    corr_matrix = corr_matrix.abs()\n",
    "\n",
    "    high_corr_var = np.where(corr_matrix > cor_threshold)\n",
    "    high_corr_var = [(corr_matrix.columns[x], corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "\n",
    "    print('There are', len(high_corr_var), 'highly corelated pairs of values')\n",
    "\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(20,13))         # Sample figsize in inches\n",
    "    # sns.heatmap(corr_matrix)\n",
    "    # plt.show()\n",
    "\n",
    "    corr_with_target = new_df_train.corr().abs()[target_col_name]\n",
    "    elements_to_exclude = []\n",
    "\n",
    "\n",
    "    for corr_pair in high_corr_var:\n",
    "        el_1 = corr_pair[0]\n",
    "        el_2 = corr_pair[1]\n",
    "        if el_1 in elements_to_exclude or el_2 in elements_to_exclude:\n",
    "            continue\n",
    "        else:\n",
    "            if corr_with_target[el_1] > corr_with_target[el_2]:\n",
    "                # print('el_1', el_1, 'el_2', el_2, 'corr_with_target[el_1]', corr_with_target[el_1], 'corr_with_target[el_2]', corr_with_target[el_2], 'удаляем ' + str(el_2))\n",
    "                elements_to_exclude.append(el_2)\n",
    "            else:\n",
    "                # print('el_1', el_1, 'el_2', el_2, 'corr_with_target[el_1]', corr_with_target[el_1], 'corr_with_target[el_2]', corr_with_target[el_2], 'удаляем ' + str(el_1))\n",
    "                elements_to_exclude.append(el_1)\n",
    "\n",
    "    print(len(elements_to_exclude), 'values will be excluded')\n",
    "    combined_data_return = combined_data.drop(columns=elements_to_exclude)\n",
    "    return combined_data_return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name='SalePrice'\n",
    "useles_col_names = ['Id']\n",
    "\n",
    "perform_target_transf = True\n",
    "exclude_anomalies = True\n",
    "misval_addit_values_dict = {}\n",
    "# misval_approach = 'kaggle_by_hand'\n",
    "misval_approach = 'threshold_elim'\n",
    "\n",
    "if misval_approach == 'threshold_elim':\n",
    "    # columns with missing values ratio more than that threshold will be dropped\n",
    "    misval_addit_values_dict['mis_prc_threshold'] = 10\n",
    "    misval_addit_values_dict['num_imp_strat'] = 'median'\n",
    "    misval_addit_values_dict['obj_imp_strat'] = 'most_frequent'\n",
    "\n",
    "add_transf_method = 'kaggle_by_hand'\n",
    "# add_transf_method = 'none'\n",
    "\n",
    "skew_threshold = 0.75\n",
    "cor_threshold = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 skewed numerical features to Box Cox transform\n",
      "There are 14 highly corelated pairs of values\n",
      "11 values will be excluded\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = init_train_test()\n",
    "df_train = transforming_train_by_hand(df_train, perform_target_transf, exclude_anomalies)\n",
    "combined_data = get_combined_data(df_train, df_test, target_col_name)\n",
    "# after this step it could be different amt of cols in combined data\n",
    "combined_data = handl_mis_vals(combined_data, misval_approach, misval_addit_values_dict)\n",
    "combined_data = data_add_transf(combined_data, add_transf_method)\n",
    "# let's remember which cols was num and which cols was object. \n",
    "# We will need this to perform some further calculations on num cols only.\n",
    "num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "combined_data = pd.get_dummies(combined_data)\n",
    "combined_data = transform_skewed_feats(combined_data, num_colnames, skew_threshold)\n",
    "combined_data = elim_cor_values(df_train, num_colnames, cor_threshold, target_col_name, combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can use different feature selection methods\n",
    "# first to come to my mind is lasso method (need to read again how to do it)\n",
    "# second thing is to train some model and take top n most important features\n",
    "# it is always better to have more methods, so I can find more later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here our goal is to build our model both neither overfitted nor underfitted\n",
    "# we need to use cros validation, different models, hyper parameter optimization and other methods to find best solution to our case\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "555d40a13b9fb3e6a6333431cc6971f2c80c0634a6d62a9e8ddc28ae8b1cedf1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
