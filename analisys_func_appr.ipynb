{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do I want here? I want everything to be in functions and like \"main\" part, that will trigger all functions above. So you can simply turn on/off any function to look at the results and also you can make changes inside functions without influencing all the code b'z it is incide the function and kinda isolated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import xgboost\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_test(train_csv_path='train.csv', test_csv_path='test.csv'):\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "    df_test = pd.read_csv(test_csv_path)\n",
    "    return df_train, df_test\n",
    "\n",
    "def transforming_train_by_hand(df_train, perform_target_transf, exclude_anomalies):\n",
    "    # in this exact case we have two houses with really huge living area and they have relatively small sale price. We need to exclude them as they are clearly outliers\n",
    "    if exclude_anomalies == True:\n",
    "        df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n",
    "    if perform_target_transf == True:\n",
    "        df_train['SalePrice'] = np.log1p(df_train[\"SalePrice\"])\n",
    "    return df_train\n",
    "\n",
    "def get_combined_data(df_train, df_test, target_col_name):\n",
    "    combined_data = pd.concat([df_train.drop(columns=target_col_name), df_test])\n",
    "    return combined_data\n",
    "\n",
    "def get_num_obj_col_names(df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        num_colnames = df.select_dtypes(include=numerics).columns\n",
    "        obj_colnames = [x for x in df.columns if x not in num_colnames]\n",
    "        return num_colnames, obj_colnames\n",
    "\n",
    "def handl_mis_vals(combined_data, misval_approach, misval_addit_values_dict):\n",
    "    if misval_approach == 'kaggle_by_hand':\n",
    "        # took it from here\n",
    "        # https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "        # starting from In [14]:\n",
    "        combined_data[\"PoolQC\"] = combined_data[\"PoolQC\"].fillna(\"None\")\n",
    "        combined_data[\"MiscFeature\"] = combined_data[\"MiscFeature\"].fillna(\"None\")\n",
    "        combined_data[\"Alley\"] = combined_data[\"Alley\"].fillna(\"None\")\n",
    "        combined_data[\"Fence\"] = combined_data[\"Fence\"].fillna(\"None\")\n",
    "        combined_data[\"FireplaceQu\"] = combined_data[\"FireplaceQu\"].fillna(\"None\")\n",
    "        # interesting way to give missing values mean values of the Neighborhood\n",
    "        combined_data[\"LotFrontage\"] = combined_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "        for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "            combined_data[col] = combined_data[col].fillna('None')\n",
    "        for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "            combined_data[col] = combined_data[col].fillna(0)\n",
    "        for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "            combined_data[col] = combined_data[col].fillna(0)\n",
    "        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "            combined_data[col] = combined_data[col].fillna('None')\n",
    "        combined_data[\"MasVnrType\"] = combined_data[\"MasVnrType\"].fillna(\"None\")\n",
    "        combined_data[\"MasVnrArea\"] = combined_data[\"MasVnrArea\"].fillna(0)\n",
    "        combined_data['MSZoning'] = combined_data['MSZoning'].fillna(combined_data['MSZoning'].mode()[0])\n",
    "        combined_data = combined_data.drop(['Utilities'], axis=1)\n",
    "        combined_data[\"Functional\"] = combined_data[\"Functional\"].fillna(\"Typ\")\n",
    "        combined_data['Electrical'] = combined_data['Electrical'].fillna(combined_data['Electrical'].mode()[0])\n",
    "        combined_data['KitchenQual'] = combined_data['KitchenQual'].fillna(combined_data['KitchenQual'].mode()[0])\n",
    "        combined_data['Exterior1st'] = combined_data['Exterior1st'].fillna(combined_data['Exterior1st'].mode()[0])\n",
    "        combined_data['Exterior2nd'] = combined_data['Exterior2nd'].fillna(combined_data['Exterior2nd'].mode()[0])\n",
    "        combined_data['SaleType'] = combined_data['SaleType'].fillna(combined_data['SaleType'].mode()[0])\n",
    "        combined_data['MSSubClass'] = combined_data['MSSubClass'].fillna(\"None\")\n",
    "\n",
    "    elif misval_approach == 'threshold_elim':\n",
    "        mis_prc_threshold = misval_addit_values_dict['mis_prc_threshold']\n",
    "\n",
    "        mis_data_df = combined_data.isnull().sum(axis=0).sort_values(ascending=False).reset_index()\n",
    "        mis_data_df.columns = ['col_name', 'cnt_missing_vals']\n",
    "        mis_data_df['mis_perc'] = mis_data_df['cnt_missing_vals'] / combined_data.shape[0]\n",
    "\n",
    "        # columns to keep due to threshold\n",
    "        cols_to_keep = list(mis_data_df[mis_data_df['mis_perc'] < (mis_prc_threshold/100)]['col_name'])\n",
    "        mis_data_threshold_df = mis_data_df[mis_data_df['col_name'].isin(cols_to_keep)]\n",
    "\n",
    "        # columns to keep due to threshold were we can still find missing values \n",
    "        cols_to_keep_with_missvals = mis_data_threshold_df[mis_data_threshold_df['mis_perc'] != 0]\n",
    "        combined_data = combined_data[cols_to_keep].copy()  \n",
    "        \n",
    "        num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "\n",
    "        # obj_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        # num_imputer = SimpleImputer(strategy='median')\n",
    "        num_imputer = SimpleImputer(strategy=misval_addit_values_dict['num_imp_strat'])\n",
    "        obj_imputer = SimpleImputer(strategy=misval_addit_values_dict['obj_imp_strat'])\n",
    "\n",
    "\n",
    "        combined_data_num_imp = pd.DataFrame(num_imputer.fit_transform(combined_data[num_colnames]), columns=combined_data[num_colnames].columns)\n",
    "        combined_data_obj_imp = pd.DataFrame(obj_imputer.fit_transform(combined_data[obj_colnames]), columns=combined_data[obj_colnames].columns)\n",
    "\n",
    "        combined_data = pd.concat([combined_data_num_imp, combined_data_obj_imp], axis=1)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def data_add_transf(combined_data, add_transf_method):\n",
    "    if add_transf_method == 'kaggle_by_hand':\n",
    "        combined_data['MSSubClass'] = combined_data['MSSubClass'].apply(str)\n",
    "        combined_data['OverallCond'] = combined_data['OverallCond'].astype(str)\n",
    "        combined_data['YrSold'] = combined_data['YrSold'].astype(str)\n",
    "        combined_data['MoSold'] = combined_data['MoSold'].astype(str)\n",
    "\n",
    "        combined_data['TotalSF'] = combined_data['TotalBsmtSF'] + combined_data['1stFlrSF'] + combined_data['2ndFlrSF']\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def transform_skewed_feats(combined_data, skew_threshold):\n",
    "    num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = combined_data[num_colnames].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "    skewness = skewness[abs(skewness) > skew_threshold]\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in [x for x in skewed_features if x != 'Id']:\n",
    "        combined_data[feat] = boxcox1p(combined_data[feat], lam)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def get_train_test_dfs_from_combined_data(df_train, target_col_name, combined_data):\n",
    "    new_df_train = combined_data[:df_train.shape[0]].copy()\n",
    "    new_df_test = combined_data[df_train.shape[0]:].copy()\n",
    "    new_df_train[target_col_name] = df_train[target_col_name]\n",
    "\n",
    "    return new_df_train, new_df_test\n",
    "\n",
    "def elim_cor_values(df_train, cor_threshold, target_col_name, combined_data):\n",
    "    num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "    num_and_targ_colnames = list(num_colnames)\n",
    "    num_and_targ_colnames.append(target_col_name)\n",
    "\n",
    "    new_df_train, new_df_test = get_train_test_dfs_from_combined_data(df_train, target_col_name, combined_data)\n",
    "\n",
    "    corr_matrix = new_df_train[num_and_targ_colnames].corr()\n",
    "\n",
    "    corr_matrix = corr_matrix.abs()\n",
    "\n",
    "    high_corr_var = np.where(corr_matrix > cor_threshold)\n",
    "    high_corr_var = [(corr_matrix.columns[x], corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "\n",
    "    print('There are', len(high_corr_var), 'highly corelated pairs of values')\n",
    "\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(20,13))         # Sample figsize in inches\n",
    "    # sns.heatmap(corr_matrix)\n",
    "    # plt.show()\n",
    "\n",
    "    corr_with_target = new_df_train.corr().abs()[target_col_name]\n",
    "    elements_to_exclude = []\n",
    "\n",
    "\n",
    "    for corr_pair in high_corr_var:\n",
    "        el_1 = corr_pair[0]\n",
    "        el_2 = corr_pair[1]\n",
    "        if el_1 in elements_to_exclude or el_2 in elements_to_exclude:\n",
    "            continue\n",
    "        else:\n",
    "            if corr_with_target[el_1] > corr_with_target[el_2]:\n",
    "                # print('el_1', el_1, 'el_2', el_2, 'corr_with_target[el_1]', corr_with_target[el_1], 'corr_with_target[el_2]', corr_with_target[el_2], 'удаляем ' + str(el_2))\n",
    "                elements_to_exclude.append(el_2)\n",
    "            else:\n",
    "                # print('el_1', el_1, 'el_2', el_2, 'corr_with_target[el_1]', corr_with_target[el_1], 'corr_with_target[el_2]', corr_with_target[el_2], 'удаляем ' + str(el_1))\n",
    "                elements_to_exclude.append(el_1)\n",
    "\n",
    "    print(len(elements_to_exclude), 'values will be excluded')\n",
    "    combined_data_return = combined_data.drop(columns=elements_to_exclude)\n",
    "    return combined_data_return\n",
    "\n",
    "def scale_combined_data(combined_data):\n",
    "    num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "    # print(num_colnames)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(combined_data[num_colnames])\n",
    "    combined_data_scaled_num_colnames = pd.DataFrame(scaled_features, index=combined_data[num_colnames].index, columns=combined_data[num_colnames].columns)\n",
    "    combined_data_1 = pd.concat([combined_data[obj_colnames], combined_data_scaled_num_colnames], axis = 0)\n",
    "    return combined_data_1\n",
    "\n",
    "def back_to_train_test(combined_data, df_train, id_field_col_name, target_col_name):\n",
    "    comb_data_with_targ = pd.merge(combined_data, df_train[[id_field_col_name, target_col_name]], how='left', on=id_field_col_name)\n",
    "    df_transf_train = comb_data_with_targ[comb_data_with_targ[target_col_name].notnull()].copy()\n",
    "    df_transf_test = comb_data_with_targ[comb_data_with_targ[target_col_name].isnull()].copy().drop(columns=[target_col_name])\n",
    "    return df_transf_train, df_transf_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name='SalePrice'\n",
    "useles_col_names = ['Id']\n",
    "id_field_col_name = 'Id'\n",
    "\n",
    "perform_target_transf = True\n",
    "exclude_anomalies = True\n",
    "misval_addit_values_dict = {}\n",
    "# misval_approach = 'kaggle_by_hand'\n",
    "misval_approach = 'threshold_elim'\n",
    "\n",
    "if misval_approach == 'threshold_elim':\n",
    "    # columns with missing values ratio more than that threshold will be dropped\n",
    "    misval_addit_values_dict['mis_prc_threshold'] = 10\n",
    "    misval_addit_values_dict['num_imp_strat'] = 'median'\n",
    "    misval_addit_values_dict['obj_imp_strat'] = 'most_frequent'\n",
    "\n",
    "add_transf_method = 'kaggle_by_hand'\n",
    "# add_transf_method = 'none'\n",
    "\n",
    "skew_threshold = 0.75\n",
    "cor_threshold = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 skewed numerical features to Box Cox transform\n",
      "There are 14 highly corelated pairs of values\n",
      "11 values will be excluded\n",
      "      MoSold_1.0  MoSold_10.0  MoSold_11.0  MoSold_12.0  MoSold_2.0  \\\n",
      "0            0.0          0.0          0.0          0.0         1.0   \n",
      "1            0.0          0.0          0.0          0.0         0.0   \n",
      "2            0.0          0.0          0.0          0.0         0.0   \n",
      "3            0.0          0.0          0.0          0.0         1.0   \n",
      "4            0.0          0.0          0.0          1.0         0.0   \n",
      "...          ...          ...          ...          ...         ...   \n",
      "5829         NaN          NaN          NaN          NaN         NaN   \n",
      "5830         NaN          NaN          NaN          NaN         NaN   \n",
      "5831         NaN          NaN          NaN          NaN         NaN   \n",
      "5832         NaN          NaN          NaN          NaN         NaN   \n",
      "5833         NaN          NaN          NaN          NaN         NaN   \n",
      "\n",
      "      MoSold_3.0  MoSold_4.0  MoSold_5.0  MoSold_6.0  MoSold_7.0  ...  \\\n",
      "0            0.0         0.0         0.0         0.0         0.0  ...   \n",
      "1            0.0         0.0         1.0         0.0         0.0  ...   \n",
      "2            0.0         0.0         0.0         0.0         0.0  ...   \n",
      "3            0.0         0.0         0.0         0.0         0.0  ...   \n",
      "4            0.0         0.0         0.0         0.0         0.0  ...   \n",
      "...          ...         ...         ...         ...         ...  ...   \n",
      "5829         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "5830         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "5831         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "5832         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "5833         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "\n",
      "      3SsnPorch  ScreenPorch  PoolArea   MiscVal   LotArea  YearBuilt  \\\n",
      "0           NaN          NaN       NaN       NaN       NaN        NaN   \n",
      "1           NaN          NaN       NaN       NaN       NaN        NaN   \n",
      "2           NaN          NaN       NaN       NaN       NaN        NaN   \n",
      "3           NaN          NaN       NaN       NaN       NaN        NaN   \n",
      "4           NaN          NaN       NaN       NaN       NaN        NaN   \n",
      "...         ...          ...       ...       ...       ...        ...   \n",
      "5829  -0.112352    -0.308318 -0.063869 -0.185835 -2.747168  -0.035834   \n",
      "5830  -0.112352    -0.308318 -0.063869 -0.185835 -2.781774  -0.035834   \n",
      "5831  -0.112352    -0.308318 -0.063869 -0.185835  1.673353  -0.365614   \n",
      "5832  -0.112352    -0.308318 -0.063869  4.868942  0.283530   0.684706   \n",
      "5833  -0.112352    -0.308318 -0.063869 -0.185835  0.119303   0.717297   \n",
      "\n",
      "      YearRemodAdd  LowQualFinSF   TotalSF  SalePrice  \n",
      "0              NaN           NaN       NaN        NaN  \n",
      "1              NaN           NaN       NaN        NaN  \n",
      "2              NaN           NaN       NaN        NaN  \n",
      "3              NaN           NaN       NaN        NaN  \n",
      "4              NaN           NaN       NaN        NaN  \n",
      "...            ...           ...       ...        ...  \n",
      "5829     -0.678305     -0.116024 -1.260545        NaN  \n",
      "5830     -0.678305     -0.116024 -1.260545        NaN  \n",
      "5831      0.564501     -0.116024  0.004721        NaN  \n",
      "5832      0.374198     -0.116024 -0.831894        NaN  \n",
      "5833      0.469390     -0.116024  0.670316        NaN  \n",
      "\n",
      "[5834 rows x 298 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjilikbaev\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1203: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = init_train_test()\n",
    "df_train = transforming_train_by_hand(df_train, perform_target_transf, exclude_anomalies)\n",
    "combined_data = get_combined_data(df_train, df_test, target_col_name)\n",
    "# after this step it could be different amt of cols in combined data\n",
    "combined_data = handl_mis_vals(combined_data, misval_approach, misval_addit_values_dict)\n",
    "combined_data = data_add_transf(combined_data, add_transf_method)\n",
    "# let's remember which cols was num and which cols was object. \n",
    "# We will need this to perform some further calculations on num cols only.\n",
    "\n",
    "combined_data = pd.get_dummies(combined_data)\n",
    "combined_data = transform_skewed_feats(combined_data, skew_threshold)\n",
    "combined_data = elim_cor_values(df_train, cor_threshold, target_col_name, combined_data)\n",
    "num_colnames, obj_colnames = get_num_obj_col_names(combined_data)\n",
    "combined_data = scale_combined_data(combined_data)\n",
    "df_train_transf, df_test_transf = back_to_train_test(combined_data, df_train, id_field_col_name, target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MoSold_1.0</th>\n",
       "      <th>MoSold_10.0</th>\n",
       "      <th>MoSold_11.0</th>\n",
       "      <th>MoSold_12.0</th>\n",
       "      <th>MoSold_2.0</th>\n",
       "      <th>MoSold_3.0</th>\n",
       "      <th>MoSold_4.0</th>\n",
       "      <th>MoSold_5.0</th>\n",
       "      <th>MoSold_6.0</th>\n",
       "      <th>MoSold_7.0</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>TotalSF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424069</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>-0.308318</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>-0.185835</td>\n",
       "      <td>-2.747168</td>\n",
       "      <td>-0.035834</td>\n",
       "      <td>-0.678305</td>\n",
       "      <td>-0.116024</td>\n",
       "      <td>-1.260545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424069</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>-0.308318</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>-0.185835</td>\n",
       "      <td>-2.781774</td>\n",
       "      <td>-0.035834</td>\n",
       "      <td>-0.678305</td>\n",
       "      <td>-0.116024</td>\n",
       "      <td>-1.260545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424069</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>-0.308318</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>-0.185835</td>\n",
       "      <td>1.673353</td>\n",
       "      <td>-0.365614</td>\n",
       "      <td>0.564501</td>\n",
       "      <td>-0.116024</td>\n",
       "      <td>0.004721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424069</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>-0.308318</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>4.868942</td>\n",
       "      <td>0.283530</td>\n",
       "      <td>0.684706</td>\n",
       "      <td>0.374198</td>\n",
       "      <td>-0.116024</td>\n",
       "      <td>-0.831894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424069</td>\n",
       "      <td>-0.112352</td>\n",
       "      <td>-0.308318</td>\n",
       "      <td>-0.063869</td>\n",
       "      <td>-0.185835</td>\n",
       "      <td>0.119303</td>\n",
       "      <td>0.717297</td>\n",
       "      <td>0.469390</td>\n",
       "      <td>-0.116024</td>\n",
       "      <td>0.670316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5834 rows × 297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MoSold_1.0  MoSold_10.0  MoSold_11.0  MoSold_12.0  MoSold_2.0  \\\n",
       "0            0.0          0.0          0.0          0.0         1.0   \n",
       "1            0.0          0.0          0.0          0.0         0.0   \n",
       "2            0.0          0.0          0.0          0.0         0.0   \n",
       "3            0.0          0.0          0.0          0.0         1.0   \n",
       "4            0.0          0.0          0.0          1.0         0.0   \n",
       "...          ...          ...          ...          ...         ...   \n",
       "5829         NaN          NaN          NaN          NaN         NaN   \n",
       "5830         NaN          NaN          NaN          NaN         NaN   \n",
       "5831         NaN          NaN          NaN          NaN         NaN   \n",
       "5832         NaN          NaN          NaN          NaN         NaN   \n",
       "5833         NaN          NaN          NaN          NaN         NaN   \n",
       "\n",
       "      MoSold_3.0  MoSold_4.0  MoSold_5.0  MoSold_6.0  MoSold_7.0  ...  \\\n",
       "0            0.0         0.0         0.0         0.0         0.0  ...   \n",
       "1            0.0         0.0         1.0         0.0         0.0  ...   \n",
       "2            0.0         0.0         0.0         0.0         0.0  ...   \n",
       "3            0.0         0.0         0.0         0.0         0.0  ...   \n",
       "4            0.0         0.0         0.0         0.0         0.0  ...   \n",
       "...          ...         ...         ...         ...         ...  ...   \n",
       "5829         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "5830         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "5831         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "5832         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "5833         NaN         NaN         NaN         NaN         NaN  ...   \n",
       "\n",
       "      EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea   MiscVal   LotArea  \\\n",
       "0               NaN        NaN          NaN       NaN       NaN       NaN   \n",
       "1               NaN        NaN          NaN       NaN       NaN       NaN   \n",
       "2               NaN        NaN          NaN       NaN       NaN       NaN   \n",
       "3               NaN        NaN          NaN       NaN       NaN       NaN   \n",
       "4               NaN        NaN          NaN       NaN       NaN       NaN   \n",
       "...             ...        ...          ...       ...       ...       ...   \n",
       "5829      -0.424069  -0.112352    -0.308318 -0.063869 -0.185835 -2.747168   \n",
       "5830      -0.424069  -0.112352    -0.308318 -0.063869 -0.185835 -2.781774   \n",
       "5831      -0.424069  -0.112352    -0.308318 -0.063869 -0.185835  1.673353   \n",
       "5832      -0.424069  -0.112352    -0.308318 -0.063869  4.868942  0.283530   \n",
       "5833      -0.424069  -0.112352    -0.308318 -0.063869 -0.185835  0.119303   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  LowQualFinSF   TotalSF  \n",
       "0           NaN           NaN           NaN       NaN  \n",
       "1           NaN           NaN           NaN       NaN  \n",
       "2           NaN           NaN           NaN       NaN  \n",
       "3           NaN           NaN           NaN       NaN  \n",
       "4           NaN           NaN           NaN       NaN  \n",
       "...         ...           ...           ...       ...  \n",
       "5829  -0.035834     -0.678305     -0.116024 -1.260545  \n",
       "5830  -0.035834     -0.678305     -0.116024 -1.260545  \n",
       "5831  -0.365614      0.564501     -0.116024  0.004721  \n",
       "5832   0.684706      0.374198     -0.116024 -0.831894  \n",
       "5833   0.717297      0.469390     -0.116024  0.670316  \n",
       "\n",
       "[5834 rows x 297 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can use different feature selection methods\n",
    "# first to come to my mind is lasso method (need to read again how to do it)\n",
    "# second thing is to train some model and take top n most important features\n",
    "# it is always better to have more methods, so I can find more later\n",
    "col_names = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here our goal is to build our model both neither overfitted nor underfitted\n",
    "# we need to use cros validation, different models, hyper parameter optimization and other methods to find best solution to our case\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "555d40a13b9fb3e6a6333431cc6971f2c80c0634a6d62a9e8ddc28ae8b1cedf1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
